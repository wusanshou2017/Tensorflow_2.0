{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "\n",
    "assert tf.__version__.startswith('2.')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor (keras.layers.Layer):\n",
    "    \"\"\"docstring for  Regressor \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.w = self.add_variable(\"meanless-name\", [13, 1])\n",
    "\n",
    "        self.b = self.add_variable(\"meanless-name\", [1])\n",
    "\n",
    "        print(self.w.shape, self.b.shape)\n",
    "        print(type(self.w), tf.is_tensor(self.w), self.w.name)\n",
    "        print(type(self.b), tf.is_tensor(self.b), self.b.name)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x = x @ self.w + self.b\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    tf.random.set_seed(22)\n",
    "\n",
    "    (x_train, y_train), (x_val, y_val) = keras.datasets.boston_housing.load_data()\n",
    "    x_train, x_val = x_train.astype(np.float32), x_val.astype(np.float32)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "\n",
    "    db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "    db_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)\n",
    "\n",
    "    model = Regressor()\n",
    "    criton = keras.losses.MeanSquaredError()\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-2)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        for stem, (x, y) in enumerate(db_train):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x)\n",
    "                logits = tf.squeeze(logits, axis=1)\n",
    "\n",
    "                loss = criton(y,logits)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        print(epoch, \"loss:\", loss.numpy())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            for x, y in db_val:\n",
    "                # [b, 1]\n",
    "                logits = model(x)\n",
    "                # [b]\n",
    "                logits = tf.squeeze(logits, axis=1)\n",
    "\n",
    "                loss = criton(y,logits)\n",
    "\n",
    "                print(epoch, \"val loss:...\", loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (404,) (102, 13) (102,)\n",
      "(13, 1) (1,)\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'> True meanless-name:0\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'> True meanless-name:0\n",
      "0 loss: 21271.105\n",
      "0 val loss:... 17932.145\n",
      "0 val loss:... 18926.053\n",
      "0 val loss:... 20703.084\n",
      "0 val loss:... 15646.309\n",
      "1 loss: 2285.5835\n",
      "2 loss: 103.70747\n",
      "3 loss: 375.60406\n",
      "4 loss: 96.48195\n",
      "5 loss: 44.10445\n",
      "6 loss: 48.416634\n",
      "7 loss: 40.98432\n",
      "8 loss: 40.567307\n",
      "9 loss: 39.982613\n",
      "10 loss: 39.620388\n",
      "10 val loss:... 83.12361\n",
      "10 val loss:... 132.5187\n",
      "10 val loss:... 141.54233\n",
      "10 val loss:... 159.5865\n",
      "11 loss: 39.14056\n",
      "12 loss: 38.597324\n",
      "13 loss: 38.06244\n",
      "14 loss: 37.50975\n",
      "15 loss: 36.923016\n",
      "16 loss: 36.31459\n",
      "17 loss: 35.695858\n",
      "18 loss: 35.06848\n",
      "19 loss: 34.43481\n",
      "20 loss: 33.79949\n",
      "20 val loss:... 71.71353\n",
      "20 val loss:... 122.58829\n",
      "20 val loss:... 126.67334\n",
      "20 val loss:... 148.968\n",
      "21 loss: 33.16602\n",
      "22 loss: 32.53678\n",
      "23 loss: 31.91413\n",
      "24 loss: 31.300177\n",
      "25 loss: 30.696606\n",
      "26 loss: 30.104824\n",
      "27 loss: 29.526066\n",
      "28 loss: 28.961304\n",
      "29 loss: 28.411402\n",
      "30 loss: 27.877039\n",
      "30 val loss:... 60.956787\n",
      "30 val loss:... 111.61479\n",
      "30 val loss:... 111.11994\n",
      "30 val loss:... 137.02206\n",
      "31 loss: 27.358784\n",
      "32 loss: 26.857082\n",
      "33 loss: 26.37229\n",
      "34 loss: 25.904678\n",
      "35 loss: 25.454412\n",
      "36 loss: 25.021631\n",
      "37 loss: 24.60638\n",
      "38 loss: 24.208647\n",
      "39 loss: 23.828367\n",
      "40 loss: 23.465445\n",
      "40 val loss:... 52.352726\n",
      "40 val loss:... 102.13422\n",
      "40 val loss:... 97.42214\n",
      "40 val loss:... 127.25854\n",
      "41 loss: 23.11971\n",
      "42 loss: 22.790977\n",
      "43 loss: 22.479008\n",
      "44 loss: 22.183533\n",
      "45 loss: 21.904266\n",
      "46 loss: 21.640873\n",
      "47 loss: 21.393024\n",
      "48 loss: 21.160343\n",
      "49 loss: 20.942469\n",
      "50 loss: 20.738977\n",
      "50 val loss:... 46.091743\n",
      "50 val loss:... 94.54772\n",
      "50 val loss:... 86.27471\n",
      "50 val loss:... 119.583015\n",
      "51 loss: 20.54948\n",
      "52 loss: 20.373549\n",
      "53 loss: 20.210772\n",
      "54 loss: 20.06071\n",
      "55 loss: 19.922941\n",
      "56 loss: 19.797018\n",
      "57 loss: 19.682516\n",
      "58 loss: 19.579006\n",
      "59 loss: 19.48605\n",
      "60 loss: 19.40323\n",
      "60 val loss:... 41.74195\n",
      "60 val loss:... 88.6539\n",
      "60 val loss:... 77.54007\n",
      "60 val loss:... 113.273125\n",
      "61 loss: 19.330128\n",
      "62 loss: 19.266329\n",
      "63 loss: 19.211416\n",
      "64 loss: 19.165007\n",
      "65 loss: 19.126715\n",
      "66 loss: 19.09615\n",
      "67 loss: 19.072948\n",
      "68 loss: 19.056755\n",
      "69 loss: 19.047215\n",
      "70 loss: 19.04399\n",
      "70 val loss:... 38.711662\n",
      "70 val loss:... 84.06175\n",
      "70 val loss:... 70.76738\n",
      "70 val loss:... 107.64805\n",
      "71 loss: 19.046764\n",
      "72 loss: 19.05521\n",
      "73 loss: 19.069033\n",
      "74 loss: 19.087933\n",
      "75 loss: 19.111637\n",
      "76 loss: 19.139874\n",
      "77 loss: 19.172377\n",
      "78 loss: 19.20891\n",
      "79 loss: 19.249233\n",
      "80 loss: 19.293114\n",
      "80 val loss:... 36.49944\n",
      "80 val loss:... 80.39436\n",
      "80 val loss:... 65.46646\n",
      "80 val loss:... 102.34501\n",
      "81 loss: 19.340343\n",
      "82 loss: 19.390705\n",
      "83 loss: 19.444012\n",
      "84 loss: 19.500072\n",
      "85 loss: 19.558708\n",
      "86 loss: 19.619762\n",
      "87 loss: 19.683054\n",
      "88 loss: 19.74844\n",
      "89 loss: 19.815777\n",
      "90 loss: 19.88493\n",
      "90 val loss:... 34.768127\n",
      "90 val loss:... 77.36461\n",
      "90 val loss:... 61.227654\n",
      "90 val loss:... 97.30914\n",
      "91 loss: 19.955755\n",
      "92 loss: 20.028149\n",
      "93 loss: 20.101984\n",
      "94 loss: 20.177145\n",
      "95 loss: 20.253536\n",
      "96 loss: 20.33105\n",
      "97 loss: 20.409601\n",
      "98 loss: 20.4891\n",
      "99 loss: 20.569447\n",
      "100 loss: 20.650581\n",
      "100 val loss:... 33.324093\n",
      "100 val loss:... 74.77804\n",
      "100 val loss:... 57.748116\n",
      "100 val loss:... 92.66259\n",
      "101 loss: 20.732418\n",
      "102 loss: 20.81488\n",
      "103 loss: 20.897915\n",
      "104 loss: 20.98144\n",
      "105 loss: 21.065395\n",
      "106 loss: 21.149738\n",
      "107 loss: 21.234394\n",
      "108 loss: 21.319326\n",
      "109 loss: 21.404474\n",
      "110 loss: 21.489788\n",
      "110 val loss:... 32.06706\n",
      "110 val loss:... 72.5089\n",
      "110 val loss:... 54.816628\n",
      "110 val loss:... 88.56986\n",
      "111 loss: 21.575216\n",
      "112 loss: 21.660732\n",
      "113 loss: 21.746286\n",
      "114 loss: 21.831835\n",
      "115 loss: 21.917336\n",
      "116 loss: 22.00277\n",
      "117 loss: 22.088076\n",
      "118 loss: 22.173233\n",
      "119 loss: 22.258213\n",
      "120 loss: 22.34298\n",
      "120 val loss:... 30.945919\n",
      "120 val loss:... 70.47517\n",
      "120 val loss:... 52.288635\n",
      "120 val loss:... 85.15359\n",
      "121 loss: 22.42751\n",
      "122 loss: 22.511763\n",
      "123 loss: 22.595716\n",
      "124 loss: 22.679354\n",
      "125 loss: 22.762623\n",
      "126 loss: 22.84552\n",
      "127 loss: 22.928026\n",
      "128 loss: 23.010107\n",
      "129 loss: 23.091736\n",
      "130 loss: 23.172901\n",
      "130 val loss:... 29.931585\n",
      "130 val loss:... 68.621124\n",
      "130 val loss:... 50.064827\n",
      "130 val loss:... 82.46391\n",
      "131 loss: 23.253588\n",
      "132 loss: 23.333767\n",
      "133 loss: 23.41342\n",
      "134 loss: 23.492535\n",
      "135 loss: 23.571089\n",
      "136 loss: 23.649082\n",
      "137 loss: 23.726477\n",
      "138 loss: 23.803274\n",
      "139 loss: 23.87946\n",
      "140 loss: 23.955004\n",
      "140 val loss:... 29.003963\n",
      "140 val loss:... 66.90763\n",
      "140 val loss:... 48.07623\n",
      "140 val loss:... 80.48388\n",
      "141 loss: 24.029913\n",
      "142 loss: 24.104185\n",
      "143 loss: 24.177773\n",
      "144 loss: 24.250713\n",
      "145 loss: 24.322987\n",
      "146 loss: 24.39455\n",
      "147 loss: 24.46543\n",
      "148 loss: 24.535618\n",
      "149 loss: 24.605099\n",
      "150 loss: 24.67387\n",
      "150 val loss:... 28.1474\n",
      "150 val loss:... 65.30661\n",
      "150 val loss:... 46.274334\n",
      "150 val loss:... 79.15012\n",
      "151 loss: 24.741936\n",
      "152 loss: 24.809286\n",
      "153 loss: 24.87592\n",
      "154 loss: 24.94183\n",
      "155 loss: 25.00703\n",
      "156 loss: 25.071514\n",
      "157 loss: 25.135277\n",
      "158 loss: 25.198336\n",
      "159 loss: 25.26067\n",
      "160 loss: 25.322308\n",
      "160 val loss:... 27.349453\n",
      "160 val loss:... 63.797894\n",
      "160 val loss:... 44.624584\n",
      "160 val loss:... 78.37558\n",
      "161 loss: 25.383245\n",
      "162 loss: 25.44347\n",
      "163 loss: 25.503\n",
      "164 loss: 25.561842\n",
      "165 loss: 25.619999\n",
      "166 loss: 25.677494\n",
      "167 loss: 25.734325\n",
      "168 loss: 25.790497\n",
      "169 loss: 25.846033\n",
      "170 loss: 25.900934\n",
      "170 val loss:... 26.600586\n",
      "170 val loss:... 62.36671\n",
      "170 val loss:... 43.10219\n",
      "170 val loss:... 78.066444\n",
      "171 loss: 25.9552\n",
      "172 loss: 26.008875\n",
      "173 loss: 26.06193\n",
      "174 loss: 26.114395\n",
      "175 loss: 26.166286\n",
      "176 loss: 26.217615\n",
      "177 loss: 26.268402\n",
      "178 loss: 26.318659\n",
      "179 loss: 26.368402\n",
      "180 loss: 26.417643\n",
      "180 val loss:... 25.893831\n",
      "180 val loss:... 61.00176\n",
      "180 val loss:... 41.68933\n",
      "180 val loss:... 78.13336\n",
      "181 loss: 26.46638\n",
      "182 loss: 26.514675\n",
      "183 loss: 26.562504\n",
      "184 loss: 26.609888\n",
      "185 loss: 26.65687\n",
      "186 loss: 26.703455\n",
      "187 loss: 26.74964\n",
      "188 loss: 26.795467\n",
      "189 loss: 26.84097\n",
      "190 loss: 26.886124\n",
      "190 val loss:... 25.224298\n",
      "190 val loss:... 59.69339\n",
      "190 val loss:... 40.373535\n",
      "190 val loss:... 78.49691\n",
      "191 loss: 26.93098\n",
      "192 loss: 26.97556\n",
      "193 loss: 27.019846\n",
      "194 loss: 27.063894\n",
      "195 loss: 27.107718\n",
      "196 loss: 27.151318\n",
      "197 loss: 27.194738\n",
      "198 loss: 27.238\n",
      "199 loss: 27.281097\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
